{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#from pyspark.sql.types import sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the convertion of pandas and spark dataframes\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf = sqlContext.read.parquet(\"redirect.parquet\")\n",
    "pageid_sdf = sqlContext.read.parquet(\"pageid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pageId: long (nullable = true)\n",
      " |-- pageTitle: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- pageId: long (nullable = true)\n",
      " |-- pageTitle: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redirect_sdf.printSchema()\n",
    "pageid_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf.registerTempTable('redirect')\n",
    "pageid_sdf.registerTempTable('pageid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df =  sqlContext.sql(\"select * from redirect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageId</th>\n",
       "      <th>pageTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Computer_accessibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>History_of_Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>Geography_of_Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Demographics_of_Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>Communications_in_Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageId                      pageTitle\n",
       "0      10         Computer_accessibility\n",
       "1      13         History_of_Afghanistan\n",
       "2      14       Geography_of_Afghanistan\n",
       "3      15    Demographics_of_Afghanistan\n",
       "4      18  Communications_in_Afghanistan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "pageId, pageTitle from redirect limit 5\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+\n",
      "|pageId|           pageTitle|__index_level_0__|\n",
      "+------+--------------------+-----------------+\n",
      "|    10| AccessibleComputing|                0|\n",
      "|    12|           Anarchism|                1|\n",
      "|    13|  AfghanistanHistory|                2|\n",
      "|    14|AfghanistanGeography|                3|\n",
      "|    15|   AfghanistanPeople|                4|\n",
      "|    18|AfghanistanCommun...|                5|\n",
      "|    19|AfghanistanTransp...|                6|\n",
      "|    20| AfghanistanMilitary|                7|\n",
      "|    21|AfghanistanTransn...|                8|\n",
      "|    23| AssistiveTechnology|                9|\n",
      "+------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT * FROM pageid LIMIT 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf = redirect_sdf.withColumnRenamed('pageTitle','fix_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf = redirect_sdf.drop('__index_level_0__')\n",
    "pageid_sdf = pageid_sdf.drop('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf = redirect_sdf.join(pageid_sdf,'pageId',how='left')\n",
    "redirect_sdf = redirect_sdf.withColumnRenamed('pageTitle','initial_target')\n",
    "redirect_sdf = redirect_sdf.drop('pageId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redirect = redirect_sdf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(fix_target='Demographics_of_Albania', initial_target='AlbaniaPeople'),\n",
       " Row(fix_target='Brachycephaly', initial_target='Brachycephalic'),\n",
       " Row(fix_target='Executive_(government)', initial_target='Executive_power'),\n",
       " Row(fix_target='Endangered_species', initial_target='Endangered_Species'),\n",
       " Row(fix_target='Evolution', initial_target='Evolved')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirect_sdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_sdf.registerTempTable('redirect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fix_target</th>\n",
       "      <th>initial_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demographics_of_Albania</td>\n",
       "      <td>AlbaniaPeople</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brachycephaly</td>\n",
       "      <td>Brachycephalic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Executive_(government)</td>\n",
       "      <td>Executive_power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Endangered_species</td>\n",
       "      <td>Endangered_Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Evolution</td>\n",
       "      <td>Evolved</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fix_target      initial_target\n",
       "0  Demographics_of_Albania       AlbaniaPeople\n",
       "1            Brachycephaly      Brachycephalic\n",
       "2   Executive_(government)     Executive_power\n",
       "3       Endangered_species  Endangered_Species\n",
       "4                Evolution             Evolved"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select * from redirect limit 5\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"\"\"\n",
    "#select * from redirect limit 10\n",
    "#\"\"\"\n",
    "#sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_sdf = sqlContext.read.parquet(\"/home/benjamin/wikipedia/Wikipedia/pagelinks_parsed0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_sdf = pagelinks_sdf.withColumnRenamed('pageTitle','target')\n",
    "pagelinks_sdf = pagelinks_sdf.drop('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn source ids to titles\n",
    "pagelinks_sdf = pagelinks_sdf.join(pageid_sdf, on='pageId', how='inner')\n",
    "pagelinks_sdf = pagelinks_sdf.withColumnRenamed('pageTitle','source')\n",
    "pagelinks_sdf = pagelinks_sdf.drop('pageId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pagelinks_sdf.persist()\n",
    "#pagelinks_sdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[target: string, source: string, fix_target: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pagelinks_sdf = pagelinks_sdf.update(redirect_sdf)\n",
    "pagelinks_sdf = pagelinks_sdf.join(redirect_sdf,pagelinks_sdf.target == redirect_sdf.initial_target,how='left')\n",
    "pagelinks_sdf =  pagelinks_sdf.drop('initial_target')\n",
    "pagelinks_sdf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(target='!HERO', source='Skillet_discography', fix_target='!Hero'),\n",
       " Row(target='(100678)_1997_XV9', source='Beijing_Schmidt_CCD_Asteroid_Program', fix_target='List_of_minor_planets:_100001–101000'),\n",
       " Row(target='(119878)_2001_CY224', source='Resonant_trans-Neptunian_object', fix_target=None),\n",
       " Row(target='(12009)_1996_UE', source='List_of_Mars-crossing_minor_planets', fix_target=None),\n",
       " Row(target='(14415)_1991_RQ7', source='Hiroshi_Kaneda', fix_target='List_of_minor_planets:_14001–15000')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagelinks_sdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source='Skillet_discography', change=True),\n",
       " Row(source='Beijing_Schmidt_CCD_Asteroid_Program', change=True),\n",
       " Row(source='Hiroshi_Kaneda', change=True),\n",
       " Row(source='List_of_Jupiter_trojans_(Greek_camp)_(1–100000)', change=True),\n",
       " Row(source='List_of_Jupiter_trojans_(Greek_camp)_(1–100000)', change=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagelinks_sdf.select('source',(pagelinks_sdf['source'] != pagelinks_sdf['target']).alias('change')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_col(l,r):\n",
    "    if r:\n",
    "        return r\n",
    "    else:\n",
    "        return l\n",
    "update_udf = udf(update_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_udf = udf(update_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`target`' given input columns: [source, fix_target];;\\n'Project ['target, fix_target#136, update_col('target, fix_target#136) AS fix_target2#176]\\n+- AnalysisBarrier\\n      +- Project [source#63, update_col(target#54, fix_target#28) AS fix_target#136]\\n         +- Project [target#54, source#63, fix_target#28]\\n            +- Join LeftOuter, (target#54 = initial_target#39)\\n               :- Project [target#54, source#63]\\n               :  +- Project [pageId#48L, target#54, pageTitle#7 AS source#63]\\n               :     +- Project [pageId#48L, target#54, pageTitle#7]\\n               :        +- Join Inner, (pageId#48L = pageId#6L)\\n               :           :- Project [pageId#48L, target#54]\\n               :           :  +- Project [pageId#48L, pageTitle#49 AS target#54, __index_level_0__#50L]\\n               :           :     +- Relation[pageId#48L,pageTitle#49,__index_level_0__#50L] parquet\\n               :           +- Project [pageId#6L, pageTitle#7]\\n               :              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\\n               +- Project [fix_target#28, initial_target#39]\\n                  +- Project [pageId#0L, fix_target#28, pageTitle#7 AS initial_target#39]\\n                     +- Project [pageId#0L, fix_target#28, pageTitle#7]\\n                        +- Join LeftOuter, (pageId#0L = pageId#6L)\\n                           :- Project [pageId#0L, fix_target#28]\\n                           :  +- Project [pageId#0L, pageTitle#1 AS fix_target#28, __index_level_0__#2L]\\n                           :     +- Relation[pageId#0L,pageTitle#1,__index_level_0__#2L] parquet\\n                           +- Project [pageId#6L, pageTitle#7]\\n                              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/softs/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/softs/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o112.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`target`' given input columns: [source, fix_target];;\n'Project ['target, fix_target#136, update_col('target, fix_target#136) AS fix_target2#176]\n+- AnalysisBarrier\n      +- Project [source#63, update_col(target#54, fix_target#28) AS fix_target#136]\n         +- Project [target#54, source#63, fix_target#28]\n            +- Join LeftOuter, (target#54 = initial_target#39)\n               :- Project [target#54, source#63]\n               :  +- Project [pageId#48L, target#54, pageTitle#7 AS source#63]\n               :     +- Project [pageId#48L, target#54, pageTitle#7]\n               :        +- Join Inner, (pageId#48L = pageId#6L)\n               :           :- Project [pageId#48L, target#54]\n               :           :  +- Project [pageId#48L, pageTitle#49 AS target#54, __index_level_0__#50L]\n               :           :     +- Relation[pageId#48L,pageTitle#49,__index_level_0__#50L] parquet\n               :           +- Project [pageId#6L, pageTitle#7]\n               :              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\n               +- Project [fix_target#28, initial_target#39]\n                  +- Project [pageId#0L, fix_target#28, pageTitle#7 AS initial_target#39]\n                     +- Project [pageId#0L, fix_target#28, pageTitle#7]\n                        +- Join LeftOuter, (pageId#0L = pageId#6L)\n                           :- Project [pageId#0L, fix_target#28]\n                           :  +- Project [pageId#0L, pageTitle#1 AS fix_target#28, __index_level_0__#2L]\n                           :     +- Relation[pageId#0L,pageTitle#1,__index_level_0__#2L] parquet\n                           +- Project [pageId#6L, pageTitle#7]\n                              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2afd19d69355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpagelinks_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fix_target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fix_target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fix_target2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/softs/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/softs/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/softs/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`target`' given input columns: [source, fix_target];;\\n'Project ['target, fix_target#136, update_col('target, fix_target#136) AS fix_target2#176]\\n+- AnalysisBarrier\\n      +- Project [source#63, update_col(target#54, fix_target#28) AS fix_target#136]\\n         +- Project [target#54, source#63, fix_target#28]\\n            +- Join LeftOuter, (target#54 = initial_target#39)\\n               :- Project [target#54, source#63]\\n               :  +- Project [pageId#48L, target#54, pageTitle#7 AS source#63]\\n               :     +- Project [pageId#48L, target#54, pageTitle#7]\\n               :        +- Join Inner, (pageId#48L = pageId#6L)\\n               :           :- Project [pageId#48L, target#54]\\n               :           :  +- Project [pageId#48L, pageTitle#49 AS target#54, __index_level_0__#50L]\\n               :           :     +- Relation[pageId#48L,pageTitle#49,__index_level_0__#50L] parquet\\n               :           +- Project [pageId#6L, pageTitle#7]\\n               :              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\\n               +- Project [fix_target#28, initial_target#39]\\n                  +- Project [pageId#0L, fix_target#28, pageTitle#7 AS initial_target#39]\\n                     +- Project [pageId#0L, fix_target#28, pageTitle#7]\\n                        +- Join LeftOuter, (pageId#0L = pageId#6L)\\n                           :- Project [pageId#0L, fix_target#28]\\n                           :  +- Project [pageId#0L, pageTitle#1 AS fix_target#28, __index_level_0__#2L]\\n                           :     +- Relation[pageId#0L,pageTitle#1,__index_level_0__#2L] parquet\\n                           +- Project [pageId#6L, pageTitle#7]\\n                              +- Relation[pageId#6L,pageTitle#7,__index_level_0__#8L] parquet\\n\""
     ]
    }
   ],
   "source": [
    "pagelinks_sdf.select('target','fix_target', update_udf('target','fix_target').alias('fix_target2')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_sdf = pagelinks_sdf.select('source', update_udf('target','fix_target').alias('fix_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_sdf.write.parquet('pagelinks_corrected0spark.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(source='Skillet_discography', fix_target='!Hero'),\n",
       " Row(source='Beijing_Schmidt_CCD_Asteroid_Program', fix_target='List_of_minor_planets:_100001–101000'),\n",
       " Row(source='Resonant_trans-Neptunian_object', fix_target='(119878)_2001_CY224'),\n",
       " Row(source='List_of_Mars-crossing_minor_planets', fix_target='(12009)_1996_UE'),\n",
       " Row(source='Hiroshi_Kaneda', fix_target='List_of_minor_planets:_14001–15000')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagelinks_sdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
